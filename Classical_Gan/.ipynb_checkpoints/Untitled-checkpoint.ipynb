{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Import_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Dummy Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [np.ones([1,4, 4,1], dtype = int) for x in range(100)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _The Generator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model(n):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(n*n*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((n, n, 256)))\n",
    "    assert model.output_shape == (None, n, n, 256) # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, n, n, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 2*n, 2*n, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 2*n, 2*n, 1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[4, 4, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator_model(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Generator Test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([1, 100])\n",
    "generated_matrix = generator(noise, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Discriminator Test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.reshape(data[0], [1,4,4,1])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator(generated_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = discriminator(generated_matrix)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Loss and Optimizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def D_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "  \n",
    "def G_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Training Functions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 100\n",
    "num_of_generated_examples = 16\n",
    "\n",
    "seed = tf.random.normal([num_of_generated_examples, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator_model(2)\n",
    "discriminator = discriminator_model()\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "#@tf.function\n",
    "def train_step(adj_matrix):\n",
    "    noise = tf.random.normal([100, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_matrix = generator(noise, training=True)\n",
    "        \n",
    "        real_output = discriminator(adj_matrix, training=True)\n",
    "        fake_output = discriminator(generated_matrix, training=True)\n",
    "\n",
    "        gen_loss = G_loss(fake_output)\n",
    "        disc_loss = D_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GAN(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        for batch in dataset:\n",
    "            train_step(batch)\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_GAN(data, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([1, 100])\n",
    "gen_2 = generator(noise, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Saving Model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save('Model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enigma = tf.keras.models.load_model('Model/', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enigma.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([1, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enigma(noise, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Adj Matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_adjacency_matrix(n):\n",
    "    \"\"\"\n",
    "    creates an nxn symmetric adjacency matrix with 0s along the diagonal\n",
    "    used to represent an undirected graph with n nodes\n",
    "    :param n: dimension\n",
    "    :return: nxn numpy array\n",
    "    \"\"\"\n",
    "    matrix = [[random.randint(0, 1) for i in range(n)] for j in range(n)]\n",
    "    # No vertex connects to itself\n",
    "    for i in range(n):\n",
    "        matrix[i][i] = 0\n",
    "    # If i is connected to j, j is connected to i\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            matrix[j][i] = matrix[i][j]\n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 1],\n",
       "       [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_adjacency_matrix(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [random_adjacency_matrix(4).reshape([1,4,4,1]) for x in range(500)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.reshape([4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Train Enigma 2 with better Dummy Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = [random_adjacency_matrix(4).reshape([1,4,4,1]) for x in range(100)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 1 is 2.7683157920837402 sec\n",
      "Time for epoch 2 is 2.6759774684906006 sec\n",
      "Time for epoch 3 is 2.647869110107422 sec\n",
      "Time for epoch 4 is 2.832427978515625 sec\n",
      "Time for epoch 5 is 2.7549357414245605 sec\n",
      "Time for epoch 6 is 2.7482006549835205 sec\n",
      "Time for epoch 7 is 2.8244383335113525 sec\n",
      "Time for epoch 8 is 2.900914192199707 sec\n",
      "Time for epoch 9 is 2.6654133796691895 sec\n",
      "Time for epoch 10 is 2.6945090293884277 sec\n",
      "Time for epoch 11 is 2.712099552154541 sec\n",
      "Time for epoch 12 is 2.6996915340423584 sec\n",
      "Time for epoch 13 is 2.6906418800354004 sec\n",
      "Time for epoch 14 is 2.680401563644409 sec\n",
      "Time for epoch 15 is 2.7673134803771973 sec\n",
      "Time for epoch 16 is 2.7330236434936523 sec\n",
      "Time for epoch 17 is 2.700709342956543 sec\n",
      "Time for epoch 18 is 2.707001209259033 sec\n",
      "Time for epoch 19 is 2.799954414367676 sec\n",
      "Time for epoch 20 is 2.703674554824829 sec\n",
      "Time for epoch 21 is 2.659414052963257 sec\n",
      "Time for epoch 22 is 2.6714985370635986 sec\n",
      "Time for epoch 23 is 2.673480272293091 sec\n",
      "Time for epoch 24 is 2.696244716644287 sec\n",
      "Time for epoch 25 is 2.6697795391082764 sec\n",
      "Time for epoch 26 is 2.6960694789886475 sec\n",
      "Time for epoch 27 is 2.6653354167938232 sec\n",
      "Time for epoch 28 is 2.649550437927246 sec\n",
      "Time for epoch 29 is 2.6762194633483887 sec\n",
      "Time for epoch 30 is 2.814218282699585 sec\n",
      "Time for epoch 31 is 2.7634220123291016 sec\n",
      "Time for epoch 32 is 2.6686439514160156 sec\n",
      "Time for epoch 33 is 2.6945273876190186 sec\n",
      "Time for epoch 34 is 2.7213077545166016 sec\n",
      "Time for epoch 35 is 2.721144437789917 sec\n",
      "Time for epoch 36 is 2.659001111984253 sec\n",
      "Time for epoch 37 is 2.692138433456421 sec\n",
      "Time for epoch 38 is 2.676579475402832 sec\n",
      "Time for epoch 39 is 2.6781184673309326 sec\n",
      "Time for epoch 40 is 2.6746084690093994 sec\n",
      "Time for epoch 41 is 2.684436559677124 sec\n",
      "Time for epoch 42 is 2.6967947483062744 sec\n",
      "Time for epoch 43 is 2.7094860076904297 sec\n",
      "Time for epoch 44 is 2.6805083751678467 sec\n",
      "Time for epoch 45 is 2.7761359214782715 sec\n",
      "Time for epoch 46 is 2.7318577766418457 sec\n",
      "Time for epoch 47 is 2.855952024459839 sec\n",
      "Time for epoch 48 is 2.666950225830078 sec\n",
      "Time for epoch 49 is 2.6837966442108154 sec\n",
      "Time for epoch 50 is 2.671522617340088 sec\n",
      "Time for epoch 51 is 2.64351224899292 sec\n",
      "Time for epoch 52 is 2.697026014328003 sec\n",
      "Time for epoch 53 is 2.704341173171997 sec\n",
      "Time for epoch 54 is 2.647260904312134 sec\n",
      "Time for epoch 55 is 2.666867256164551 sec\n",
      "Time for epoch 56 is 2.702779531478882 sec\n",
      "Time for epoch 57 is 2.708024263381958 sec\n",
      "Time for epoch 58 is 2.674276351928711 sec\n",
      "Time for epoch 59 is 2.660513401031494 sec\n",
      "Time for epoch 60 is 2.8732638359069824 sec\n",
      "Time for epoch 61 is 2.768671989440918 sec\n",
      "Time for epoch 62 is 2.688234329223633 sec\n",
      "Time for epoch 63 is 2.6781792640686035 sec\n",
      "Time for epoch 64 is 2.869795799255371 sec\n",
      "Time for epoch 65 is 2.7441678047180176 sec\n",
      "Time for epoch 66 is 2.680093765258789 sec\n",
      "Time for epoch 67 is 2.7174441814422607 sec\n",
      "Time for epoch 68 is 2.6894822120666504 sec\n",
      "Time for epoch 69 is 2.6746773719787598 sec\n",
      "Time for epoch 70 is 2.6654748916625977 sec\n",
      "Time for epoch 71 is 2.6819164752960205 sec\n",
      "Time for epoch 72 is 2.6443347930908203 sec\n",
      "Time for epoch 73 is 2.7523295879364014 sec\n",
      "Time for epoch 74 is 2.6352338790893555 sec\n",
      "Time for epoch 75 is 2.861837863922119 sec\n",
      "Time for epoch 76 is 2.7002127170562744 sec\n",
      "Time for epoch 77 is 2.647928476333618 sec\n",
      "Time for epoch 78 is 2.6588127613067627 sec\n",
      "Time for epoch 79 is 2.6931276321411133 sec\n",
      "Time for epoch 80 is 2.7459754943847656 sec\n",
      "Time for epoch 81 is 2.7230334281921387 sec\n",
      "Time for epoch 82 is 2.69057297706604 sec\n",
      "Time for epoch 83 is 2.640547513961792 sec\n",
      "Time for epoch 84 is 2.6793575286865234 sec\n",
      "Time for epoch 85 is 2.639770269393921 sec\n",
      "Time for epoch 86 is 2.702421188354492 sec\n",
      "Time for epoch 87 is 2.6644058227539062 sec\n",
      "Time for epoch 88 is 2.6712005138397217 sec\n",
      "Time for epoch 89 is 2.641594409942627 sec\n",
      "Time for epoch 90 is 2.7646677494049072 sec\n",
      "Time for epoch 91 is 2.6839911937713623 sec\n",
      "Time for epoch 92 is 2.714028835296631 sec\n",
      "Time for epoch 93 is 2.73602294921875 sec\n",
      "Time for epoch 94 is 2.778794765472412 sec\n",
      "Time for epoch 95 is 2.769043207168579 sec\n",
      "Time for epoch 96 is 2.7026283740997314 sec\n",
      "Time for epoch 97 is 2.7260477542877197 sec\n",
      "Time for epoch 98 is 2.806011438369751 sec\n",
      "Time for epoch 99 is 2.780900716781616 sec\n",
      "Time for epoch 100 is 2.799619674682617 sec\n"
     ]
    }
   ],
   "source": [
    "train_GAN(data_2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model/assets\n"
     ]
    }
   ],
   "source": [
    "generator.save('Model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "enigma = tf.keras.models.load_model('Model/', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 1024)              102400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 2, 2, 128)         819200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 2, 2, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 4, 4, 64)          204800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 4, 4, 1)           1600      \n",
      "=================================================================\n",
      "Total params: 1,132,864\n",
      "Trainable params: 1,130,432\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enigma.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([1, 100])\n",
    "cat = enigma(noise, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 4, 1), dtype=float32, numpy=\n",
       "array([[[[-0.00650752],\n",
       "         [ 0.24790123],\n",
       "         [ 0.31918916],\n",
       "         [ 0.04215904]],\n",
       "\n",
       "        [[ 0.33064163],\n",
       "         [ 0.0209253 ],\n",
       "         [ 0.82438385],\n",
       "         [ 0.2521356 ]],\n",
       "\n",
       "        [[ 0.15727054],\n",
       "         [ 0.84887254],\n",
       "         [ 0.04201905],\n",
       "         [ 0.921328  ]],\n",
       "\n",
       "        [[-0.0090999 ],\n",
       "         [ 0.24500865],\n",
       "         [ 0.907346  ],\n",
       "         [-0.01367382]]]], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.reshape(cat, [4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[-0.00650752,  0.24790123,  0.31918916,  0.04215904],\n",
       "       [ 0.33064163,  0.0209253 ,  0.82438385,  0.2521356 ],\n",
       "       [ 0.15727054,  0.84887254,  0.04201905,  0.921328  ],\n",
       "       [-0.0090999 ,  0.24500865,  0.907346  , -0.01367382]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo = tf.math.greater(test, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 1],\n",
       "       [0, 0, 1, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(bo, tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
